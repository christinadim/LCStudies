{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification of ATLAS Calorimeter Topo-Clusters: Let's Try 2D CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.8\n"
     ]
    }
   ],
   "source": [
    "#import libraries and some constants\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, LogNorm\n",
    "import pandas as pd\n",
    "import uproot as ur\n",
    "import atlas_mpl_style as ampl\n",
    "ampl.use_atlas_style()\n",
    "print(ur.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = '/gpfs/share/home/cdimitri/ML4Pions/LCStudies/'\n",
    "plotpath = path_prefix+'classifier/Plots/'\n",
    "modelpath = path_prefix+'classifier/Models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import our resolution utilities\n",
    "\n",
    "import sys\n",
    "sys.path.append(path_prefix)\n",
    "\n",
    "from  util import resolution_util as ru\n",
    "from  util import plot_util as pu\n",
    "from  util import ml_util as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath = '/cephfs/user/cdimitri/ML4Pions/'\n",
    "rootfiles = [\"pi0_subset\", \"pi_charged_subset\"]\n",
    "\n",
    "trees, pdata = mu.setupPionData(inputpath, rootfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pi0 events: 1243504\n",
      "Number of pi+/- events: 1261440\n",
      "Total: 2504944\n"
     ]
    }
   ],
   "source": [
    "np0 = len(pdata['pi0_subset'])\n",
    "nppm = len(pdata['pi_charged_subset'])\n",
    "\n",
    "print(\"Number of pi0 events: {}\".format(np0))\n",
    "print(\"Number of pi+/- events: {}\".format(nppm))\n",
    "print(\"Total: {}\".format(np0+nppm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells = {\n",
    "    ifile : {\n",
    "        layer : mu.setupCells(itree, layer, flatten = False)\n",
    "        for layer in mu.cell_meta\n",
    "    }\n",
    "    for ifile, itree in trees.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we train the CNN still?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "NUM_THREADS=1\n",
    "tensorflow.config.threading.set_inter_op_parallelism_threads(NUM_THREADS)\n",
    "tensorflow.config.threading.set_intra_op_parallelism_threads(NUM_THREADS)\n",
    "\n",
    "gpu_list = [\"/gpu:0\"]\n",
    "#strategy = tf.distribute.MirroredStrategy(devices=gpu_list)\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "#ngpu = strategy.num_replicas_in_sync\n",
    "#print ('Number of devices: {}'.format(ngpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_classes = ['pi0_subset','pi_charged_subset']\n",
    "# create train/validation/test subsets containing 70%/10%/20%\n",
    "# of events from each type of pion event\n",
    "pdata_merged, pcells_merged, plabels = mu.createTrainingDatasets(training_classes, pdata, pcells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_merged_reshaped = mu.reshapeSeparateCNN(pcells_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure how useful these 1,1 filters are?\n",
    "# may need 'shallower' networks instead for the further networks\n",
    "filters = {\n",
    "    'EMB1': (2,4), \n",
    "    'EMB2': (4,4), \n",
    "    'EMB3': (4,2), \n",
    "    'TileBar0': (2,2), \n",
    "    'TileBar1': (2,2), \n",
    "    'TileBar2': (1,1)\n",
    "}\n",
    "pools2 = {\n",
    "    'EMB1': (2,2), \n",
    "    'EMB2': (2,2), \n",
    "    'EMB3': (1,1), \n",
    "    'TileBar0': (1,1), \n",
    "    'TileBar1': (1,1), \n",
    "    'TileBar2': (1,1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_layers(layer):\n",
    "    print(layer)\n",
    "    # create model\n",
    "    #with strategy.scope():\n",
    "    model = Sequential()\n",
    "    #model.add(Convolution2D(32, filters[layer], input_shape=(1,mu.cell_meta[layer]['len_eta'],mu.cell_meta[layer]['len_phi']), activation='relu', data_format = 'channels_last'))\n",
    "    model.add(Convolution2D(32, filters[layer], input_shape=(mu.cell_meta[layer]['len_eta'],mu.cell_meta[layer]['len_phi'],1),activation='relu', data_format = 'channels_first'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    # model.add(Convolution2D(16, (2, 2), activation='relu'))\n",
    "    model.add(Convolution2D(16, pools2[layer], activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=pools2[layer]))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(2, kernel_initializer='normal', activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMB1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 4 from 1 for '{{node conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d/Conv2D/ReadVariableOp)' with input shapes: [?,128,4,1], [2,4,128,32].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1852\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 4 from 1 for '{{node conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d/Conv2D/ReadVariableOp)' with input shapes: [?,128,4,1], [2,4,128,32].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d29727960d86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m models = {\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcnn_model_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_meta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m }\n",
      "\u001b[0;32m<ipython-input-14-d29727960d86>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m models = {\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcnn_model_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_meta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m }\n",
      "\u001b[0;32m<ipython-input-13-2af8773b16e1>\u001b[0m in \u001b[0;36mcnn_model_layers\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#model.add(Convolution2D(32, filters[layer], input_shape=(1,mu.cell_meta[layer]['len_eta'],mu.cell_meta[layer]['len_phi']), activation='relu', data_format = 'channels_last'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConvolution2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'len_eta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'len_phi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'channels_first'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPool2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# model.add(Convolution2D(16, (2, 2), activation='relu'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    206\u001b[0m           \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m           \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m           \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m           \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                                 input_list)\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1088\u001b[0m           layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[1;32m   1091\u001b[0m             inputs, input_masks, args, kwargs)\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m     name=None):\n\u001b[0;32m-> 1013\u001b[0;31m   return convolution_internal(\n\u001b[0m\u001b[1;32m   1014\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m       \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m       return op(\n\u001b[0m\u001b[1;32m   1144\u001b[0m           \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2595\u001b[0m     \u001b[0;31m# We avoid calling squeeze_batch_dims to reduce extra python function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2596\u001b[0m     \u001b[0;31m# call slowdown in eager mode.  This branch doesn't require reshapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2597\u001b[0;31m     return gen_nn_ops.conv2d(\n\u001b[0m\u001b[1;32m   2598\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \"'conv2d' Op, not %r.\" % dilations)\n\u001b[1;32m    968\u001b[0m   \u001b[0mdilations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dilations\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdilations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m    970\u001b[0m         \u001b[0;34m\"Conv2D\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    746\u001b[0m       \u001b[0;31m# Add Op to graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0m\u001b[1;32m    749\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                                  attrs=attr_protos, op_def=op_def)\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    588\u001b[0m       \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         compute_device)\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3526\u001b[0m     \u001b[0;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3528\u001b[0;31m       ret = Operation(\n\u001b[0m\u001b[1;32m   3529\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3530\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2013\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mop_def\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2014\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2015\u001b[0;31m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0m\u001b[1;32m   2016\u001b[0m                                 control_input_ops, op_def)\n\u001b[1;32m   2017\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pool/condor/dir_50803/ml4pions/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1854\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1856\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 4 from 1 for '{{node conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d/Conv2D/ReadVariableOp)' with input shapes: [?,128,4,1], [2,4,128,32]."
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    layer: cnn_model_layers(layer)\n",
    "    for layer in mu.cell_meta\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    layer: models[layer].fit(pcells_merged_reshaped[layer][pdata_merged.train], \n",
    "                                plabels[pdata_merged.train], \n",
    "                                validation_data=(pcells_merged_reshaped[layer][pdata_merged.val], plabels[pdata_merged.val]), epochs=100, batch_size=200, verbose=2)\n",
    "    for layer in mu.cell_meta\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {}\n",
    "scores = {}\n",
    "for layer in mu.cell_meta:\n",
    "    print('On layer ' + layer)\n",
    "    \n",
    "    # get overall performance metric\n",
    "    performance[layer] = models[layer].evaluate(\n",
    "        pcells_merged_reshaped[layer][pdata_merged.test], plabels[pdata_merged.test],\n",
    "        verbose = 0,\n",
    "    )\n",
    "    \n",
    "    # get network scores for the dataset\n",
    "    scores[layer] = models[layer].predict(\n",
    "        pcells_merged_reshaped[layer]\n",
    "    )\n",
    "    \n",
    "    print('Finished layer ' + layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "for layer, model in models.items():\n",
    "    model.save(modelpath+\"model_cnn_\"+layer+\".h5\")\n",
    "    with open(modelpath + \"model_cnn_\"+layer+\".history\",'wb') as model_history_file:\n",
    "        pickle.dump(history[layer].history, model_history_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "models = {}\n",
    "history = {}\n",
    "for layer in mu.cell_meta:\n",
    "    models[layer] = tf.keras.models.load_model(modelpath+\"model_cnn_\"+layer+\".h5\")\n",
    "    with open(modelpath + 'model_cnn_'+layer+'.history','rb') as model_history_file:\n",
    "        history[laye] = pickle.load(model_history_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in mu.cell_meta:\n",
    "#     print(history_flat[layer_i].history.keys())\n",
    "    plt.cla(); plt.clf()\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    plt.plot(history[layer].history['acc'])\n",
    "    plt.plot(history[layer].history['val_acc'])\n",
    "    plt.title('model accuracy for ' + layer)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    # plt.savefig('Plots/accuracy_' + layer + '.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # summarize history for loss\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(history[layer].history['loss'])\n",
    "    plt.plot(history[layer].history['val_loss'])\n",
    "    plt.title('model loss for ' + layer)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    # plt.savefig(plotpath + 'loss_' + layer + '.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "roc_fpr = {}\n",
    "roc_tpr = {}\n",
    "roc_thresh = {}\n",
    "roc_auc = {}\n",
    "\n",
    "for layer in mu.cell_meta:\n",
    "    roc_fpr[layer], roc_tpr[layer], roc_thresh[layer] = roc_curve(\n",
    "        plabels[pdata_merged.test][:,1],\n",
    "        scores[layer][pdata_merged.test,1],\n",
    "        drop_intermediate=False,\n",
    "    )\n",
    "    roc_auc[layer] = auc(roc_fpr[layer], roc_tpr[layer])\n",
    "    print('Area under curve for ' + layer + ': ' + str(roc_auc[layer]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu.roc_plot([roc_fpr[layer] for layer in mu.cell_meta],\n",
    "            [roc_tpr[layer] for layer in mu.cell_meta],\n",
    "            figfile=plotpath + 'roc_cnn_all.pdf',\n",
    "            labels=['{} (area = {:.3f})'.format(layer, roc_auc[layer]) for layer in mu.cell_meta],\n",
    "            extra_lines=[[[0, 1], [0, 1]]],\n",
    "            title='CNN ROC curve: classification of $\\pi^+$ vs. $\\pi^0$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More with CNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_model_emb123():\n",
    "    with strategy.scope():\n",
    "        # EMB1 image (flat, fully-connected)\n",
    "        input1 = Input(shape=(1, 128, 4), name='input1')\n",
    "        x1 = Convolution2D(32, (4, 2), activation='relu', data_format = 'channels_first')(input1)\n",
    "        x1 = MaxPool2D(pool_size=(2, 2))(x1)\n",
    "        x1 = Dropout(0.2)(x1)\n",
    "        x1 = Flatten()(x1)\n",
    "        x1 = Dense(128, activation='relu')(x1)\n",
    "\n",
    "        # EMB2 image (convolutional)\n",
    "        input2 = Input(shape=(1,16,16), name='input2')\n",
    "        x2 = Convolution2D(32, (4, 4), activation='relu', data_format = 'channels_first')(input2)\n",
    "        x2 = MaxPool2D(pool_size=(2, 2))(x2)\n",
    "        x2 = Dropout(0.2)(x2)\n",
    "        x2 = Flatten()(x2)\n",
    "        x2 = Dense(128, activation='relu')(x2)\n",
    "    \n",
    "        # EMB3 image (convolutional)\n",
    "        input3 = Input(shape=(1,8,16), name='input3')\n",
    "        x3 = Convolution2D(32, (2, 4), activation='relu', data_format = 'channels_first')(input3)\n",
    "        x3 = MaxPool2D(pool_size=(1, 2))(x3)\n",
    "        x3 = Dropout(0.2)(x3)\n",
    "        x3 = Flatten()(x3)\n",
    "        x3 = Dense(128, activation='relu')(x3)\n",
    "\n",
    "        # concatenate outputs from the three networks above\n",
    "        x = concatenate([x1, x2, x3]) \n",
    "        x = Dense(50, activation='relu')(x)    \n",
    "\n",
    "        # final output\n",
    "        output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs = [input1, input2, input3], outputs = [output])\n",
    "    \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])    \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_emb123 = merged_model_emb123()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(merged_emb123, plotpath+'emb123_with_shape_info.png', show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_emb123 = merged_emb123.fit([pcells_merged_reshaped['EMB1'][pdata_merged.train],\n",
    "                                    pcells_merged_reshaped['EMB2'][pdata_merged.train],\n",
    "                                    pcells_merged_reshaped['EMB3'][pdata_merged.train]],\n",
    "                                    plabels[pdata_merged.train], \n",
    "                                    validation_data=([pcells_merged_reshaped['EMB1'][pdata_merged.val],\n",
    "                                                        pcells_merged_reshaped['EMB2'][pdata_merged.val],\n",
    "                                                        pcells_merged_reshaped['EMB3'][pdata_merged.val]], \n",
    "                                                        plabels[pdata_merged.val]),                            \n",
    "                                    epochs=250, batch_size=200*ngpu, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_emb123.save(modelpath+'model_emb123.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.cla(); plt.clf()\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    plt.plot(history_emb123.history['accuracy'])\n",
    "    plt.plot(history_emb123.history['val_accuracy'])\n",
    "    plt.title('model accuracy for EMB123')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    # plt.savefig('Plots/accuracy_' + layer + '.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # summarize history for loss\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(history_emb123.history['loss'])\n",
    "    plt.plot(history_emb123.history['val_loss'])\n",
    "    plt.title('model loss for emb123')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    # plt.savefig(plotpath + 'loss_' + layer + '.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_123 = merged_emb123.predict(\n",
    "        [pcells_merged_reshaped['EMB1'], pcells_merged_reshaped['EMB2'], pcells_merged_reshaped['EMB3']]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "roc_fpr_123, roc_tpr_123, roc_thresh_123 = roc_curve(\n",
    "        plabels[pdata_merged.test][:,1],\n",
    "        scores_123[pdata_merged.test,1],\n",
    "        drop_intermediate=False,\n",
    "    )\n",
    "roc_auc_123 = auc(roc_fpr_123, roc_tpr_123)\n",
    "print('Area under curve for EMB123 ' + str(roc_auc_123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla(); plt.clf()\n",
    "# fig, ax = plt.subplots(1, 2, tight_layout=True, figsize=(10,4))\n",
    "# fig.patch.set_facecolor('white')\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(roc_fpr_123, roc_tpr_123, label='CNN (area = {:.3f})'.format(roc_auc_123))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('CNN ROC curve: classification of $\\pi^+$ vs. $\\pi^0$')\n",
    "plt.legend(loc='best')\n",
    "# plt.savefig('Plots/roc_lc_only.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_model_all():\n",
    "    with strategy.scope():\n",
    "        # EMB1 image (convolutional)\n",
    "        input1 = Input(shape=(1, 128, 4), name='input1')\n",
    "        x1 = Convolution2D(32, (4, 2), activation='relu', data_format = 'channels_first')(input1)\n",
    "        x1 = MaxPool2D(pool_size=(2, 2))(x1)\n",
    "        x1 = Dropout(0.2)(x1)\n",
    "        x1 = Flatten()(x1)\n",
    "        x1 = Dense(128, activation='relu')(x1)\n",
    "\n",
    "        # EMB2 image (convolutional)\n",
    "        input2 = Input(shape=(1,16,16), name='input2')\n",
    "        x2 = Convolution2D(32, (4, 4), activation='relu', data_format = 'channels_first')(input2)\n",
    "        x2 = MaxPool2D(pool_size=(2, 2))(x2)\n",
    "        x2 = Dropout(0.2)(x2)\n",
    "        x2 = Flatten()(x2)\n",
    "        x2 = Dense(128, activation='relu')(x2)\n",
    "    \n",
    "        # EMB3 image (convolutional)\n",
    "        input3 = Input(shape=(1,8,16), name='input3')\n",
    "        x3 = Convolution2D(32, (2, 4), activation='relu', data_format = 'channels_first')(input3)\n",
    "        x3 = MaxPool2D(pool_size=(2, 2))(x3)\n",
    "        x3 = Dropout(0.2)(x3)\n",
    "        x3 = Flatten()(x3)\n",
    "        x3 = Dense(128, activation='relu')(x3)\n",
    "\n",
    "        input4 = Input(shape=(1,4,4), name='input4')\n",
    "        x4 = Convolution2D(32, (2,2), activation='relu', data_format = 'channels_first')(input4)\n",
    "        x4 = MaxPool2D(pool_size=(2,2))(x4)\n",
    "        x4 = Dropout(0.2)(x4)\n",
    "        x4 = Flatten()(x4)\n",
    "        x4 = Dense(128, activation='relu')(x4)\n",
    "\n",
    "        input5 = Input(shape=(1,4,4), name='input5')\n",
    "        x5 = Convolution2D(32, (2,2), activation='relu', data_format = 'channels_first')(input5)\n",
    "        x5 = MaxPool2D(pool_size=(2,2))(x5)\n",
    "        x5 = Dropout(0.2)(x5)\n",
    "        x5 = Flatten()(x5)\n",
    "        x5 = Dense(128, activation='relu')(x5)\n",
    "\n",
    "        input6 = Input(shape=(1,2,4), name='input6')\n",
    "        x6 = Convolution2D(32, (2,2), activation='relu', data_format = 'channels_first')(input6)\n",
    "        x6 = MaxPool2D(pool_size=(2,1))(x6)\n",
    "        x6 = Dropout(0.2)(x6)\n",
    "        x6 = Flatten()(x6)\n",
    "        x6 = Dense(128, activation='relu')(x6)\n",
    "\n",
    "        # concatenate outputs from the three networks above\n",
    "        x = concatenate([x1, x2, x3, x4, x5, x6]) \n",
    "        x = Dense(50, activation='relu')(x)    \n",
    "\n",
    "        # final output\n",
    "        output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs = [input1, input2, input3, input4, input5, input6], outputs = [output])\n",
    "    \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])    \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all = merged_model_all()\n",
    "merged_all.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(merged_all, plotpath+'all_with_shape_info.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_all = merged_all.fit([pcells_merged_reshaped['EMB1'][pdata_merged.train],\n",
    "                                    pcells_merged_reshaped['EMB2'][pdata_merged.train],\n",
    "                                    pcells_merged_reshaped['EMB3'][pdata_merged.train],\n",
    "                                    pcells_merged_reshaped['TileBar0'][pdata_merged.train],\n",
    "                                    pcells_merged_reshaped['TileBar1'][pdata_merged.train],\n",
    "                                    pcells_merged_reshaped['TileBar2'][pdata_merged.train]],\n",
    "                                    plabels[pdata_merged.train], \n",
    "                                    validation_data=([pcells_merged_reshaped['EMB1'][pdata_merged.val],\n",
    "                                            pcells_merged_reshaped['EMB2'][pdata_merged.val],\n",
    "                                            pcells_merged_reshaped['EMB3'][pdata_merged.val],\n",
    "                                            pcells_merged_reshaped['TileBar0'][pdata_merged.val],\n",
    "                                            pcells_merged_reshaped['TileBar1'][pdata_merged.val],\n",
    "                                            pcells_merged_reshaped['TileBar2'][pdata_merged.val]], \n",
    "                                            plabels[pdata_merged.val]),                            \n",
    "                                    epochs=250, batch_size=200*ngpu, verbose=2)\n",
    "merged_all.save(modelpath+'model_cnn_all.h5')\n",
    "scores_all = merged_all.predict(\n",
    "        [pcells_merged_reshaped['EMB1'], pcells_merged_reshaped['EMB2'], pcells_merged_reshaped['EMB3'],pcells_merged_reshaped['TileBar0'], pcells_merged_reshaped['TileBar1'], pcells_merged_reshaped['TileBar2']]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "roc_fpr_all, roc_tpr_all, roc_thresh_all = roc_curve(\n",
    "        plabels[pdata_merged.test][:,1],\n",
    "        scores_all[pdata_merged.test,1],\n",
    "        drop_intermediate=False,\n",
    "    )\n",
    "roc_auc_all = auc(roc_fpr_all, roc_tpr_all)\n",
    "print('Area under curve for CNN All ' + str(roc_auc_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla(); plt.clf()\n",
    "# fig, ax = plt.subplots(1, 2, tight_layout=True, figsize=(10,4))\n",
    "# fig.patch.set_facecolor('white')\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(roc_fpr_123, roc_tpr_123, label='CNN EMB (area = {:.3f})'.format(roc_auc_123))\n",
    "plt.plot(roc_fpr_all, roc_tpr_all, label='CNN all (area = {:.3f})'.format(roc_auc_all))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('CNN ROC curve: classification of $\\pi^+$ vs. $\\pi^0$')\n",
    "plt.legend(loc='best')\n",
    "# plt.savefig('Plots/roc_lc_only.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, now we play with more complicated models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterEMB2 = (zoom(cluster) for cluster in pcells_merged['EMB2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iterEMB2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upscaledEMB3 = scipy.ndimage.zoom(pcells_merged['EMB3'], (1,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upscaledEMB3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downScaledEMB1 = scipy.ndimage.zoom(pcells_merged['EMB1'], (1,0.125,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downScaledEMB1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_EMB2G_channels = mu.setupChannelImages(mu.rescaleImages(pcells_merged, (16, 16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_EMB2G_channels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channels_EMB2G_model_all():\n",
    "    with strategy.scope():\n",
    "        # Here come the 16x16 inputs\n",
    "        input1 = Input(shape=(6, 16, 16), name='input1')\n",
    "        x1 = Convolution2D(32, (4, 4), activation='relu', data_format = 'channels_first')(input1)\n",
    "        x1 = MaxPool2D(pool_size=(2, 2))(x1)\n",
    "        x1 = Dropout(0.2)(x1)\n",
    "        x1 = Flatten()(x1)\n",
    "        x1 = Dense(128, activation='relu')(x1)\n",
    "\n",
    "\n",
    "        # final output\n",
    "        output = Dense(2, activation='softmax')(x1)\n",
    "\n",
    "        model = Model(inputs = [input1], outputs = [output])\n",
    "    \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])    \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_channels = channels_EMB2G_model_all()\n",
    "model_channels.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_channels = model_channels.fit([pcells_EMB2G_channels[pdata_merged.train]],\n",
    "                                    plabels[pdata_merged.train], \n",
    "                                    validation_data=([pcells_EMB2G_channels[pdata_merged.val]], \n",
    "                                            plabels[pdata_merged.val]),                            \n",
    "                                    epochs=250, batch_size=200*ngpu, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_channels.save(modelpath+'model_cnn_channels.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_chan = model_channels.predict(\n",
    "        [pcells_EMB2G_channels]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "roc_fpr_chan, roc_tpr_chan, roc_thresh_chan = roc_curve(\n",
    "        plabels[pdata_merged.test][:,1],\n",
    "        scores_chan[pdata_merged.test,1],\n",
    "        drop_intermediate=False,\n",
    "    )\n",
    "roc_auc_chan = auc(roc_fpr_chan, roc_tpr_chan)\n",
    "print('Area under curve for CNN All ' + str(roc_auc_chan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.981 for no drop-out, but lots of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.cla(); plt.clf()\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    plt.plot(history_channels.history['accuracy'])\n",
    "    plt.plot(history_channels.history['val_accuracy'])\n",
    "    plt.title('model accuracy for Channels')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    # plt.savefig('Plots/accuracy_' + layer + '.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # summarize history for loss\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(history_channels.history['loss'])\n",
    "    plt.plot(history_channels.history['val_loss'])\n",
    "    plt.title('model loss for Channels')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    # plt.savefig(plotpath + 'loss_' + layer + '.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test the largest granularity possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_GG_channels = mu.setupChannelImages(mu.rescaleImages(pcells_merged, (128, 16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channels_GG_model_all():\n",
    "    with strategy.scope():\n",
    "        # Here come the 16x16 inputs\n",
    "        input1 = Input(shape=(6, 128, 16), name='input1')\n",
    "        x1 = Convolution2D(32, (4, 4), activation='relu', data_format = 'channels_first')(input1)\n",
    "        x1 = MaxPool2D(pool_size=(2, 2))(x1)\n",
    "        x1 = Dropout(0.2)(x1)\n",
    "        x1 = Flatten()(x1)\n",
    "        x1 = Dense(128, activation='relu')(x1)\n",
    "\n",
    "\n",
    "        # final output\n",
    "        output = Dense(2, activation='softmax')(x1)\n",
    "\n",
    "        model = Model(inputs = [input1], outputs = [output])\n",
    "    \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])    \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok that doesn't work because it's too big :D or at least I'm not patient enough\n",
    "\n",
    "Instead let's try something simpler..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_EMB2_channels = mu.setupChannelImages(mu.rescaleImages(pcells_merged, (16, 16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_merged['EMB1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_EMB1_flat = pcells_merged['EMB1'].reshape(len(pcells_merged['EMB1']), 128 * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channels_EMB2G_EMB1F_model_all():\n",
    "    with strategy.scope():\n",
    "        # Here come the 16x16 inputs\n",
    "        input1 = Input(shape=(6, 16, 16), name='input1')\n",
    "        x1 = Convolution2D(32, (4, 4), activation='relu', data_format = 'channels_first')(input1)\n",
    "        x1 = MaxPool2D(pool_size=(2, 2))(x1)\n",
    "        x1 = Dropout(0.2)(x1)\n",
    "        x1 = Flatten()(x1)\n",
    "        x1 = Dense(128, activation='relu')(x1)\n",
    "\n",
    "        emb1_dim = 128*4\n",
    "        #Here's EMB1 flattened\n",
    "        input2 = Input(shape=(emb1_dim), name='input2')\n",
    "        x2 = Dense(emb1_dim, activation='relu')(input2)\n",
    "        x2 = Dropout(0.2)(x2)\n",
    "        x2 = Dense(emb1_dim / 2, activation='relu')(x2)\n",
    "        x2 = Dropout(0.2)(x2)\n",
    "        x2 = Dense(emb1_dim / 4, activation='relu')(x2)\n",
    "\n",
    "        # concatenate outputs from the two networks above\n",
    "        x = concatenate([x1, x2]) \n",
    "        x = Dense(50, activation='relu')(x)    \n",
    "\n",
    "        # final output\n",
    "        output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs = [input1,input2], outputs = [output])\n",
    "    \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])    \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_EMB2G_EMB1F = channels_EMB2G_EMB1F_model_all()\n",
    "model_EMB2G_EMB1F.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_EMB2G_EMB1F = model_EMB2G_EMB1F.fit([pcells_EMB2_channels[pdata_merged.train], pcells_EMB1_flat[pdata_merged.train]],\n",
    "                                    plabels[pdata_merged.train], \n",
    "                                    validation_data=([pcells_EMB2_channels[pdata_merged.val], pcells_EMB1_flat[pdata_merged.val]], \n",
    "                                            plabels[pdata_merged.val]),                            \n",
    "                                    epochs=250, batch_size=200*ngpu, verbose=2)\n",
    "model_EMB2G_EMB1F.save(modelpath+'model_EMB2G_EMB1F.h5')\n",
    "scores_EMB2G_EMB1F = model_EMB2G_EMB1F.predict([pcells_EMB2_channels, pcells_EMB1_flat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.cla(); plt.clf()\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    plt.plot(history_EMB2G_EMB1F.history['accuracy'])\n",
    "    plt.plot(history_EMB2G_EMB1F.history['val_accuracy'])\n",
    "    plt.title('model accuracy for Channels')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    # plt.savefig('Plots/accuracy_' + layer + '.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # summarize history for loss\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(history_EMB2G_EMB1F.history['loss'])\n",
    "    plt.plot(history_EMB2G_EMB1F.history['val_loss'])\n",
    "    plt.title('model loss for Channels')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    # plt.savefig(plotpath + 'loss_' + layer + '.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "roc_fpr_EMB2G_EMB1F, roc_tpr_EMB2G_EMB1F, roc_thresh_EMB2G_EMB1F = roc_curve(\n",
    "        plabels[pdata_merged.test][:,1],\n",
    "        scores_EMB2G_EMB1F[pdata_merged.test,1],\n",
    "        drop_intermediate=False,\n",
    "    )\n",
    "roc_auc_EMB2G_EMB1F = auc(roc_fpr_EMB2G_EMB1F, roc_tpr_EMB2G_EMB1F)\n",
    "print('Area under curve for CNN EMB2G_EMB1F ' + str(roc_auc_EMB2G_EMB1F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla(); plt.clf()\n",
    "# fig, ax = plt.subplots(1, 2, tight_layout=True, figsize=(10,4))\n",
    "# fig.patch.set_facecolor('white')\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(roc_fpr_EMB2G_EMB1F, roc_tpr_EMB2G_EMB1F, label='CNN EMB2G EMB1F (area = {:.3f})'.format(roc_auc_EMB2G_EMB1F))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('CNN ROC curve: classification of $\\pi^+$ vs. $\\pi^0$')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(plotpath+'/roc_EMB2G_EMB1F_only.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channels_EMB2G_EMB1F_model_all2():\n",
    "    with strategy.scope():\n",
    "        # Here come the 16x16 inputs\n",
    "        input1 = Input(shape=(6, 16, 16), name='input1')\n",
    "        x1 = Convolution2D(32, (4, 4), activation='relu', data_format = 'channels_first')(input1)\n",
    "        # x1 = MaxPool2D(pool_size=(2, 2))(x1)\n",
    "        x1 = Flatten()(x1)\n",
    "        x1 = Dense(128, activation='relu')(x1)\n",
    "        x1 = Dropout(0.2)(x1)\n",
    "        x1 = Dense(64, activation='relu')(x1)\n",
    "\n",
    "        emb1_dim = 128*4\n",
    "        #Here's EMB1 flattened\n",
    "        input2 = Input(shape=(emb1_dim), name='input2')\n",
    "        x2 = Dense(emb1_dim, activation='relu')(input2)\n",
    "        x2 = Dropout(0.2)(x2)\n",
    "        x2 = Dense(emb1_dim / 2, activation='relu')(x2)\n",
    "        x2 = Dropout(0.2)(x2)\n",
    "        x2 = Dense(emb1_dim / 4, activation='relu')(x2)\n",
    "        x2 = Dropout(0.2)(x2)\n",
    "        x2 = Dense(emb1_dim / 8, activation='relu')(x2)\n",
    "\n",
    "        # concatenate outputs from the two networks above\n",
    "        x = concatenate([x1, x2]) \n",
    "        x = Dense(50, activation='relu')(x)    \n",
    "\n",
    "        # final output\n",
    "        output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs = [input1,input2], outputs = [output])\n",
    "    \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])    \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_EMB2G_EMB1F_2 = channels_EMB2G_EMB1F_model_all2()\n",
    "model_EMB2G_EMB1F_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_EMB2G_EMB1F_2 = model_EMB2G_EMB1F_2.fit([pcells_EMB2_channels[pdata_merged.train], pcells_EMB1_flat[pdata_merged.train]],\n",
    "                                    plabels[pdata_merged.train], \n",
    "                                    validation_data=([pcells_EMB2_channels[pdata_merged.val], pcells_EMB1_flat[pdata_merged.val]], \n",
    "                                            plabels[pdata_merged.val]),                            \n",
    "                                    epochs=250, batch_size=200*ngpu, verbose=2)\n",
    "model_EMB2G_EMB1F_2.save(modelpath+'model_EMB2G_EMB1F_2.h5')\n",
    "scores_EMB2G_EMB1F_2 = model_EMB2G_EMB1F_2.predict([pcells_EMB2_channels, pcells_EMB1_flat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "roc_fpr_EMB2G_EMB1F_2, roc_tpr_EMB2G_EMB1F_2, roc_thresh_EMB2G_EMB1F_2 = roc_curve(\n",
    "        plabels[pdata_merged.test][:,1],\n",
    "        scores_EMB2G_EMB1F_2[pdata_merged.test,1],\n",
    "        drop_intermediate=False,\n",
    "    )\n",
    "roc_auc_EMB2G_EMB1F_2 = auc(roc_fpr_EMB2G_EMB1F_2, roc_tpr_EMB2G_EMB1F_2)\n",
    "print('Area under curve for CNN EMB2G_EMB1F 2 ' + str(roc_auc_EMB2G_EMB1F_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channels_EMB2G_EMB1F_model_all3():\n",
    "    with strategy.scope():\n",
    "        # Here come the 16x16 inputs\n",
    "        input1 = Input(shape=(6, 16, 16), name='input1')\n",
    "        x1 = Convolution2D(32, (4, 4), activation='relu', data_format = 'channels_first')(input1)\n",
    "        # x1 = MaxPool2D(pool_size=(2, 2))(x1)\n",
    "        x1 = Flatten()(x1)\n",
    "        x1 = Dense(128, activation='relu')(x1)\n",
    "        x1 = Dropout(0.2)(x1)\n",
    "        x1 = Dense(64, activation='relu')(x1)\n",
    "\n",
    "        # this layer corresponds to physical granularity of EMB2\n",
    "        x1_EMB2 = MaxPool2D(pool_size=(2, 1))(input1)\n",
    "        x1_EMB2 = Convolution2D(32, (2, 2), activation='relu', data_format = 'channels_first')(x1_EMB2)\n",
    "        x1_EMB2 = Flatten()(x1_EMB2)\n",
    "        x1_EMB2 = Dense(128, activation='relu')(x1_EMB2)\n",
    "        x1_EMB2 = Dropout(0.2)(x1_EMB2)\n",
    "        x1_EMB2 = Dense(64, activation='relu')(x1_EMB2)\n",
    "\n",
    "        # this layer corresponds to physical granularity of TileBar0\n",
    "        x1_T0 = MaxPool2D(pool_size=(2, 2))(input1)\n",
    "        x1_T0 = Convolution2D(32, (2, 2), activation='relu', data_format = 'channels_first')(x1_T0)\n",
    "        x1_T0 = Flatten()(x1_T0)\n",
    "        x1_T0 = Dense(128, activation='relu')(x1_T0)\n",
    "        x1_T0 = Dropout(0.2)(x1_T0)\n",
    "        x1_T0 = Dense(64, activation='relu')(x1_T0)\n",
    "\n",
    "        emb1_dim = 128*4\n",
    "        #Here's EMB1 flattened\n",
    "        input2 = Input(shape=(emb1_dim), name='input2')\n",
    "        x2 = Dense(emb1_dim, activation='relu')(input2)\n",
    "        x2 = Dropout(0.2)(x2)\n",
    "        x2 = Dense(emb1_dim / 2, activation='relu')(x2)\n",
    "        x2 = Dropout(0.2)(x2)\n",
    "        x2 = Dense(emb1_dim / 4, activation='relu')(x2)\n",
    "        x2 = Dropout(0.2)(x2)\n",
    "        x2 = Dense(emb1_dim / 8, activation='relu')(x2)\n",
    "\n",
    "        # concatenate outputs from the two networks above\n",
    "        x = concatenate([x1,x1_EMB2,x1_T0, x2]) \n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.2)(x)    \n",
    "        x = Dense(64, activation='relu')(x)\n",
    "\n",
    "        # final output\n",
    "        output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs = [input1,input2], outputs = [output])\n",
    "    \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])    \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_EMB2G_EMB1F_3 = channels_EMB2G_EMB1F_model_all3()\n",
    "model_EMB2G_EMB1F_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_EMB2G_EMB1F_3 = model_EMB2G_EMB1F_3.fit([pcells_EMB2_channels[pdata_merged.train], pcells_EMB1_flat[pdata_merged.train]],\n",
    "                                    plabels[pdata_merged.train], \n",
    "                                    validation_data=([pcells_EMB2_channels[pdata_merged.val], pcells_EMB1_flat[pdata_merged.val]], \n",
    "                                            plabels[pdata_merged.val]),                            \n",
    "                                    epochs=250, batch_size=200*ngpu, verbose=2)\n",
    "model_EMB2G_EMB1F_3.save(modelpath+'model_EMB2G_EMB1F_3.h5')\n",
    "scores_EMB2G_EMB1F_3 = model_EMB2G_EMB1F_3.predict([pcells_EMB2_channels, pcells_EMB1_flat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "roc_fpr_EMB2G_EMB1F_3, roc_tpr_EMB2G_EMB1F_3, roc_thresh_EMB2G_EMB1F_3 = roc_curve(\n",
    "        plabels[pdata_merged.test][:,1],\n",
    "        scores_EMB2G_EMB1F_3[pdata_merged.test,1],\n",
    "        drop_intermediate=False,\n",
    "    )\n",
    "roc_auc_EMB2G_EMB1F_3 = auc(roc_fpr_EMB2G_EMB1F_3, roc_tpr_EMB2G_EMB1F_3)\n",
    "print('Area under curve for CNN EMB2G_EMB1F 3 ' + str(roc_auc_EMB2G_EMB1F_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.cla(); plt.clf()\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    plt.plot(history_EMB2G_EMB1F_3.history['accuracy'])\n",
    "    plt.plot(history_EMB2G_EMB1F_3.history['val_accuracy'])\n",
    "    plt.title('model accuracy for Channels')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    # plt.savefig('Plots/accuracy_' + layer + '.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # summarize history for loss\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(history_EMB2G_EMB1F_3.history['loss'])\n",
    "    plt.plot(history_EMB2G_EMB1F_3.history['val_loss'])\n",
    "    plt.title('model loss for Channels')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    # plt.savefig(plotpath + 'loss_' + layer + '.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channels_EMB2G_EMB1F_model_all4():\n",
    "    with strategy.scope():\n",
    "        # Here come the 16x16 inputs\n",
    "        input1 = Input(shape=(6, 16, 16), name='input1')\n",
    "\n",
    "        x1 = Convolution2D(32, (4, 4), activation='relu', data_format = 'channels_first')(input1)\n",
    "        x1 = MaxPool2D(pool_size=(2, 2))(x1)\n",
    "        x1 = Convolution2D(32, (2, 2), activation='relu')(x1)\n",
    "        x1 = MaxPool2D(pool_size=(2, 2))(x1)\n",
    "        x1 = Flatten()(x1)\n",
    "        x1 = Dense(128, activation='relu')(x1)\n",
    "        x1 = Dropout(0.2)(x1)\n",
    "        x1 = Dense(64, activation='relu')(x1)\n",
    "\n",
    "        x1_2 = Convolution2D(64, (2, 2), activation='relu', data_format = 'channels_first')(input1)\n",
    "        x1_2 = MaxPool2D(pool_size=(2, 2))(x1_2)\n",
    "        x1_2 = Convolution2D(32, (2, 2), activation='relu')(x1_2)\n",
    "        x1_2 = MaxPool2D(pool_size=(2, 2))(x1_2)\n",
    "        x1_2 = Flatten()(x1_2)\n",
    "        x1_2 = Dense(128, activation='relu')(x1_2)\n",
    "        x1_2 = Dropout(0.2)(x1_2)\n",
    "        x1_2 = Dense(64, activation='relu')(x1_2)\n",
    "\n",
    "        emb1_dim = 128*4\n",
    "        #Here's EMB1 flattened\n",
    "        input2 = Input(shape=(emb1_dim), name='input2')\n",
    "        x2 = Dense(emb1_dim, activation='relu')(input2)\n",
    "        x2 = Dropout(0.2)(x2)\n",
    "        x2 = Dense(emb1_dim / 2, activation='relu')(x2)\n",
    "        x2 = Dropout(0.2)(x2)\n",
    "        x2 = Dense(emb1_dim / 4, activation='relu')(x2)\n",
    "        x2 = Dropout(0.2)(x2)\n",
    "        x2 = Dense(emb1_dim / 8, activation='relu')(x2)\n",
    "\n",
    "        # concatenate outputs from the two networks above\n",
    "        x = concatenate([x1, x1_2, x2]) \n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.2)(x)    \n",
    "        x = Dense(64, activation='relu')(x)  \n",
    "\n",
    "        # final output\n",
    "        output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs = [input1,input2], outputs = [output])\n",
    "    \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])    \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_EMB2G_EMB1F_4 = channels_EMB2G_EMB1F_model_all4()\n",
    "model_EMB2G_EMB1F_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_EMB2G_EMB1F_4 = model_EMB2G_EMB1F_4.fit([pcells_EMB2_channels[pdata_merged.train], pcells_EMB1_flat[pdata_merged.train]],\n",
    "                                    plabels[pdata_merged.train], \n",
    "                                    validation_data=([pcells_EMB2_channels[pdata_merged.val], pcells_EMB1_flat[pdata_merged.val]], \n",
    "                                            plabels[pdata_merged.val]),                            \n",
    "                                    epochs=250, batch_size=200*ngpu, verbose=2)\n",
    "model_EMB2G_EMB1F_4.save(modelpath+'model_EMB2G_EMB1F_4.h5')\n",
    "scores_EMB2G_EMB1F_4 = model_EMB2G_EMB1F_4.predict([pcells_EMB2_channels, pcells_EMB1_flat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, from the accuracy it's clear this isn't beating 2, which is our best network. Ok, let's try RESNET?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESNET TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from https://keras.io/examples/cifar10_resnet/\n",
    "# modified to use tf.keras layer names\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Convolution2D(num_filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=strides,\n",
    "                    padding='same',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, final_pool = 8, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=final_pool)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet_v2(input_shape, final_pool, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=final_pool)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_EMB2_channels_last = mu.setupChannelImages(mu.rescaleImages(pcells_merged, (16, 16)), last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = pcells_EMB2_channels_last.shape[1:]\n",
    "batch_size = 128  # orig paper trained all networks with batch_size=128\n",
    "epochs = 100\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 2\n",
    "n = 5\n",
    "if version == 1:\n",
    "    depth = n * 6 + 2\n",
    "elif version == 2:\n",
    "    depth = n * 9 + 2\n",
    "model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "if version == 2:\n",
    "    model_ResNet47v2 = resnet_v2(input_shape=input_shape, final_pool=4, depth=depth, num_classes=num_classes)\n",
    "else:\n",
    "    model_ResNet47v2 = resnet_v1(input_shape=input_shape, final_pool=4, depth=depth, num_classes=num_classes)\n",
    "\n",
    "model_ResNet47v2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "model_ResNet47v2.summary()\n",
    "print(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_ResNet47v2 = model_ResNet47v2.fit([pcells_EMB2_channels_last[pdata_merged.train]],\n",
    "                                    plabels[pdata_merged.train], \n",
    "                                    validation_data=([pcells_EMB2_channels_last[pdata_merged.val]], \n",
    "                                            plabels[pdata_merged.val]),                            \n",
    "                                    epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "model_ResNet47v2.save(modelpath+'model_ResNet47v2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ResNet47v2 = tf.keras.models.load_model(modelpath+'model_ResNet47v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ResNet20v1 = model.predict([pcells_EMB2_channels_last])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ResNet47v2 = model_ResNet47v2.predict([pcells_EMB2_channels_last])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "roc_fpr_ResNet20v1, roc_tpr_ResNet20v1, roc_thresh_ResNet20v1 = roc_curve(\n",
    "        plabels[pdata_merged.test][:,1],\n",
    "        scores_ResNet20v1[pdata_merged.test,1],\n",
    "        drop_intermediate=False,\n",
    "    )\n",
    "roc_auc_ResNet20v1 = auc(roc_fpr_ResNet20v1, roc_tpr_ResNet20v1)\n",
    "print('Area under curve for ResNet20v1 ' + str(roc_auc_ResNet20v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "roc_fpr_ResNet47v2, roc_tpr_ResNet47v2, roc_thresh_ResNet47v2 = roc_curve(\n",
    "        plabels[pdata_merged.train][:,1],\n",
    "        scores_ResNet47v2[pdata_merged.train,1],\n",
    "        drop_intermediate=False,\n",
    "    )\n",
    "roc_auc_ResNet47v2 = auc(roc_fpr_ResNet47v2, roc_tpr_ResNet47v2)\n",
    "print('Area under curve for ResNet47v2 ' + str(roc_auc_ResNet47v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.cla(); plt.clf()\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    plt.plot(history_ResNet20v1.history['accuracy'])\n",
    "    plt.plot(history_ResNet20v1.history['val_accuracy'])\n",
    "    plt.title('model accuracy for Channels')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    # plt.savefig('Plots/accuracy_' + layer + '.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # summarize history for loss\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(history_ResNet20v1.history['loss'])\n",
    "    plt.plot(history_ResNet20v1.history['val_loss'])\n",
    "    plt.title('model loss for Channels')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    # plt.savefig(plotpath + 'loss_' + layer + '.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla(); plt.clf()\n",
    "# fig, ax = plt.subplots(1, 2, tight_layout=True, figsize=(10,4))\n",
    "# fig.patch.set_facecolor('white')\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(roc_fpr_ResNet20v1, roc_tpr_ResNet20v1, label='ResNet20v1 (area = {:.3f})'.format(roc_auc_ResNet20v1))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ResNet20v1 ROC curve: classification of $\\pi^+$ vs. $\\pi^0$')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(plotpath+'/roc_ResNet20v1.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
